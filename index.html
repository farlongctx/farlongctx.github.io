<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Long-Context Autoregressive Video Modeling with Next-Frame Prediction">
  <meta name="keywords" content="FAR">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>FAR</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">
  <link href='https://fonts.googleapis.com/css?family=Caveat' rel='stylesheet'>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Scrollable Table with Arrow Buttons</title>
    <style>
      .video-no-container video::-webkit-media-controls {
        display: none !important;
      }
    </style>
  </head>

<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">Long-Context Autoregressive Video Modeling <br/> with Next-Frame Prediction</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://ycgu.site/">Yuchao Gu</a>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=S7bGBmkyNtEC&hl=zh-CN/">Weijia Mao</a>,
            </span>
            <span class="author-block">
              <a href="https://sites.google.com/view/showlab">Mike Zheng Shou</a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">ShowLab, National University of Singapore</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">              
              <span class="link-block">
                <a href="https://arxiv.org/abs/2503.19325"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/showlab/FAR"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body is-centered has-text-centered">
      <p style="background-color:lightgoldenrodyellow">
      ðŸ“–<strong>TL;DR</strong>: FAR (i.e., <strong><u>F</u></strong>rame <strong><u>A</u></strong>uto<strong><u>R</u></strong>egressive Models) is a new baseline for <a style="color: purple; font-weight: bold;">autoregressive video generation</a>, and achieves state-of-the-art performance on both short- and long-context video modeling. 
      </p>
    </div>
  </div> 
</section>
<!-- End teaser video -->

<section class="section hero is-light">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Long-context autoregressive modeling has significantly advanced language generation, but video generation still struggles to fully utilize extended temporal contexts. To investigate long-context video modeling, we introduce Frame AutoRegressive (FAR), a strong baseline for video autoregressive modeling. Just as language models learn causal dependencies between tokens (i.e., Token AR), FAR models temporal causal dependencies between continuous frames, achieving better convergence than Token AR and video diffusion transformers. Building on FAR, we observe that long-context video modeling faces challenges due to visual redundancy. Training on long videos is computationally expensive, as vision tokens grow much faster than language tokens. To tackle this issue, we propose balancing locality and long-range dependency through long short-term context modeling. A high-resolution short-term context window ensures fine-grained temporal consistency, while an unlimited long-term context window encodes long-range information using fewer tokens. With this approach, we can train on long video sequences with a manageable token context length, thereby significantly reducing training time and memory usage. Furthermore, we propose a multi-level KV cache designed to support the long short-term context modeling, which accelerating inference on long video sequences. We demonstrate that FAR achieves state-of-the-art performance in both short- and long-video generation, providing a simple yet effective baseline for video autoregressive modeling.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->    
</section>

<!-- Paper poster -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">What is the potential of FAR compared to video diffusion transformers?</h2>
          <div class="content has-text-justified">
              
            <strong>1. Better Convergence:</strong> 
              <ul>
                <li>FAR requires same training cost to video diffusion transformers.</li>
                <li>FAR achieves better convergence than video diffusion transformers with the same latent space.</li>
              </ul>

            <div style="text-align: center;">
              <img src="static/assets/converenge.jpg" alt="" width="400" />
            </div>
            
            <hr>

            <strong>2. Native Support for Vision Context:</strong>
              <ul>
                <li>Video diffusion transformers: requires additional image-to-video fine-tuning to expolit image conditions.</li>
                <li>FAR: provides native support for clean vision context at various lengths, achieving state-of-the-art performance in video generation (context frame = 0) and video prediction (context frame â‰¥ 1).</li>
              </ul>

            <div style="text-align: center;">
              <img src="static/assets/performance.png" alt="" width="800" />
            </div>
            
            <hr>

            <strong>3. Efficient Training on Long Video Sequence:</strong>
            
            <ul>
              <li>Video diffusion transformers: cannot efficiently train on long videos because the vision token scales rapidly with the number of frames.</li>
              <li>FAR: exploits <b>long short-term context modeling</b>, reduce redundant token lengths during training or fine-tuning on long videos.</li>
            </ul>

            <div style="text-align: center;">
              <img src="static/assets/long_short_term_ctx.png" alt="" width="1000" />
            </div>

            <hr>

            <strong>4. Fast Autoregressive Inference on Long Video Sequence:</strong>

            <ul>
              <li>Video diffusion transformers: cannot directly autoregressive inference for long-video without image-to-video finetuning.</li>
              <li>FAR: exploits <b>multi-level kv cache</b> to speedup autoregressive inference on long videos.</li>
            </ul>

            <div style="text-align: center;">
              <img src="static/assets/kv_cache_comp.png" alt="" width="400" />
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!--End paper poster -->

<section class="section hero is-light">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Pipeline</h2>
        <div class="content has-text-justified">

          <div style="text-align: center;">
            <img src="static/assets/pipeline.png" alt="" width="1000" />
          </div>

          <strong>FAR is trained using a frame-wise flow-matching objective with autoregressive contexts. The attention mask in FAR ðŸ‘‡ preserves causality between frames while allowing full attention within each frame.</strong>
          
          <br/><br/>
          
          <div style="text-align: center;">
            <img src="static/assets/attn_mask.png" alt="" width="400" />
          </div>

          <strong>Multi-level KV cacheðŸ‘‡ is adopted in FAR for speedup autoregressive inference on long video.</strong>
          
          <br/><br/>
          
          <div style="text-align: center;">
            <img src="static/assets/multi_level_kv_cache.png" alt="" width="1000" />
          </div>
          
          
          <br/>
          <strong>Key Techniques:</strong>
          <ol>
            <li> <strong>Stochastic Clean Context:</strong> In training, we stochastically replace a portion of noisy context with clean context frames and use timesteps beyond the diffusion scheduler (e.g., -1) to indicate them. This strategy bridges the training and inference gap of observed contexts without incurring additional training costs. </li>
            <li> <strong>Long Short-Term Context Modeling:</strong> To reduce the training cost on long-video sequences, we maintain a high-resolution short-term context window to model fine-grained temporal consistency, and an unlimited long-term context window to reduce redundant tokens with <b>aggressive patchification</b>. </li>
            <li> <strong>Multi-Level KV Cache:</strong> To speedup autoregressive inference on long-video sequences, we schedule long-term and short-term context with multi-level KV cache. </li>
          </ol>

        </div>
      </div>
    </div>
  </div>
    <!--/ Abstract. -->    
</section>

<!-- Paper poster -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3">Main Results</h2>
          <div class="content has-text-justified">

            <strong>1. Compared to previous methods, FAR effectively exploits the provided context frames <a style="color: red; font-weight: bold;">(annotated with red boxes)</a> and achieves long-prediction consistency (on 3D structures and wall's patterns).</strong>
            
            <br/><br/>

            <table width="1200" align="center" class="video-no-container">
              <tbody>
                <tr>
                  <th><video id="master15" width="200" src="static/assets/dmlab_samples/sample_gt_0.mp4" autoplay="" loop="" controls="" muted=""> </video> </th>
                  <th><video data-master-id="master15" width="200" src="static/assets/dmlab_samples/sample_gt_1.mp4" autoplay="" loop="" controls="" muted=""> </video> </th>
                  <th><video data-master-id="master15" width="200" src="static/assets/dmlab_samples/sample_gt_2.mp4" autoplay="" loop="" controls="" muted=""> </video> </th>
                  <th><video data-master-id="master15" width="200" src="static/assets/dmlab_samples/sample_gt_3.mp4" autoplay="" loop="" controls="" muted=""> </video> </th>
                  <th><video data-master-id="master15" width="200" src="static/assets/dmlab_samples/sample_gt_5.mp4" autoplay="" loop="" controls="" muted=""> </video> </th>
                  <th><video data-master-id="master15" width="200" src="static/assets/dmlab_samples/sample_gt_7.mp4" autoplay="" loop="" controls="" muted=""> </video> </th>
                </tr>
              </tbody>
            </table>

            <p style="text-align: center; margin-top: 10px;"><strong>(red boxes: observed context frames, left: prediction, right: ground-truth)</strong></p>

            <div style="text-align: center;">
              <img src="static/assets/dmlab_sample.png" alt="" width="1000" />
            </div>

            <hr>

            <strong>2. FAR achieves state-of-the-art performance on unconditional/conditional video generation, short-video prediction and long-video preidction.</strong>
            
            <br/><br/>

            <div style="text-align: center;">
              <img src="static/assets/result.png" alt="" width="1000" />
            </div>

            <hr>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!--End paper poster -->

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{gu2025long,
      title={Long-Context Autoregressive Video Modeling with Next-Frame Prediction},
      author={Gu, Yuchao and Mao, weijia and Shou, Mike Zheng},
      journal={arXiv preprint arXiv:2503.19325},
      year={2025}
}</code></pre>
  </div>
</section>


<footer class="footer">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>


<script>
  window.onload = function () {
    // Sync function
    function syncVideos(master, slaves) {
      master.addEventListener('play', () => {
        slaves.forEach(video => {
          video.currentTime = master.currentTime;
          video.play();
        });
      });

      master.addEventListener('pause', () => {
        slaves.forEach(video => video.pause());
      });

      master.addEventListener('seeked', () => {
        slaves.forEach(video => {
          video.currentTime = master.currentTime;
        });
      });
    }

    // Sync setup for all masters
    document.querySelectorAll('[id^="master"]').forEach(masterVideo => {
      const masterId = masterVideo.getAttribute('id');
      const slaveVideos = Array.from(document.querySelectorAll(`[data-master-id="${masterId}"]`));
      syncVideos(masterVideo, slaveVideos);
    });
  };
  
</script>

</body>
</html>
