<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Long-Context Autoregressive Video Modeling with Next-Frame Prediction">
  <meta name="keywords" content="FAR">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>FAR</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">
  <link href='https://fonts.googleapis.com/css?family=Caveat' rel='stylesheet'>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Scrollable Table with Arrow Buttons</title>
    <style>
      .video-no-container video::-webkit-media-controls {
        display: none !important;
      }
    </style>
  </head>

<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">Long-Context Autoregressive Video Modeling <br/> with Next-Frame Prediction</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://ycgu.site/">Yuchao Gu</a>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=S7bGBmkyNtEC&hl=zh-CN/">Weijia Mao</a>,
            </span>
            <span class="author-block">
              <a href="https://sites.google.com/view/showlab">Mike Zheng Shou</a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">ShowLab, National University of Singapore</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">              
              <span class="link-block">
                <a href="https://arxiv.org/abs/2312.02087"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/showlab/FAR"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body is-centered has-text-centered">
      <p style="background-color:lightgoldenrodyellow">
      ðŸ“–<strong>TL;DR</strong>: FAR (<u>i.e.</u>, <strong><u>F</u></strong>rame <strong><u>A</u></strong>utoregressive <strong><u>M</u></strong>odels) is a new baseline for <a style="color: purple; font-weight: bold;">autoregressive video generation</a>, and achieves state-of-the-art performance on both short- and long-context video modeling. 
      </p>
    </div>
  </div> 
</section>
<!-- End teaser video -->

<section class="section hero is-light">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Long-context autoregressive modeling has significantly advanced language generation, but video generation still struggles to fully utilize extended temporal contexts. To investigate long-context video modeling, we introduce Frame AutoRegressive (FAR), a strong baseline for video autoregressive modeling. Just as language models learn causal dependencies between tokens (i.e., Token AR), FAR models temporal causal dependencies between continuous frames, achieving better convergence than Token AR and video diffusion transformers. Building on FAR, we observe that long-context vision modeling faces challenges due to visual redundancy. Existing RoPE lacks effective temporal decay for remote context and fails to extrapolate well to long video sequences. Additionally, training on long videos is computationally expensive, as vision tokens grow much faster than language tokens. To tackle these issues, we propose balancing locality and long-range dependency. We introduce FlexRoPE, an test-time technique that adds flexible temporal decay to RoPE, enabling extrapolation to 16Ã— longer vision contexts. Furthermore, we propose long short-term context modeling, where a high-resolution short-term context window ensures fine-grained temporal consistency, while an unlimited long-term context window encodes long-range information using fewer tokens. With this approach, we can train on long video sequences with a manageable token context length. We demonstrate that FAR achieves state-of-the-art performance in both short- and long-video generation, providing a simple yet effective baseline for video autoregressive modeling.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->    
</section>

<!-- Paper poster -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">What is the potential of FAR compared to video diffusion transformers?</h2>
          <div class="content has-text-justified">
              
            <strong>1. Better Converenge:</strong> 
              <ul>
                <li>FAR requires same training cost to video diffusion transformers.</li>
                <li>FAR achieves better convergence than video diffusion transformers with the same latent space.</li>
              </ul>

            <div style="text-align: center;">
              <img src="static/assets/converenge.jpg" alt="" width="400" />
            </div>
            
            <hr>

            <strong>2. Native Support for Vision Context:</strong>
              <ul>
                <li>Video diffusion transformers: requires additional image-to-video fine-tuning to expolit image conditions.</li>
                <li>FAR: provides native support for clean vision context at various lengths, achieving state-of-the-art performance in video generation (context frame = 0) and video prediction (context frame â‰¥ 1).</li>
              </ul>

            <div style="text-align: center;">
              <img src="static/assets/performance.png" alt="" width="800" />
            </div>
            
            <hr>
            
            <strong>3. Test-time Temporal Extrapolation:</strong> 
              <ul>
                <li>Video diffusion transformers: are usually not suitable to generate longer sequence than their training window.</li>
                <li>FAR: support for 16x longer test-time temporal extrapolation without fine-tuning on long video sequence.</li>
              </ul>
            

            <div style="text-align: center;">
              <img src="static/assets/extrapolation.png" alt="" width="1000" />
            </div>

            <hr>

            <strong>4. Efficient Training on Long Video Sequence:</strong>
            
            <ul>
              <li>Video diffusion transformers: cannot efficient train on long videos.</li>
              <li>FAR: exploits long short-term context modeling, reduce the redundant token lengths when training/fine-tuning on long videos.</li>
            </ul>

            <div style="text-align: center;">
              <img src="static/assets/long_short_term_ctx.jpg" alt="" width="400" />
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!--End paper poster -->

<section class="section hero is-light">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Pipeline</h2>
        <div class="content has-text-justified">

          <div style="text-align: center;">
            <img src="static/assets/pipeline.png" alt="" width="1000" />
          </div>
          
          <strong>Key Techniques:</strong>
          <ol>
            <li> <strong>Stochastic Clean Context:</strong> In training, we stochastically replace a portion of noisy context with clean context frames and use timesteps beyond the diffusion scheduler (e.g., -1) to indicate them. This strategy bridges the training and inference gap of observed contexts without incurring additional training costs. </li>
            <li> <strong>Long Short-Term Context Modeling:</strong> In training/fine-tuning on long-video sequences, we maintain a high-resolution short-term context window to model fine-grained temporal consistency, and an unlimited long-term context window to reduce redundant tokens with aggressive patchification. </li>
          </ol>

        </div>
      </div>
    </div>
  </div>
    <!--/ Abstract. -->    
</section>

<!-- Paper poster -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3">Main Results</h2>
          <div class="content has-text-justified">

            <strong>1. Compared to previous methods, FAR effectively exploits the provided context frames <a style="color: red; font-weight: bold;">(annotated with red boxes)</a> and achieves long-prediction consistency (on 3D structures and wall's patterns).</strong>
            
            <br/><br/>

            <table width="1200" align="center" class="video-no-container">
              <tbody>
                <tr>
                  <th><video id="master15" width="200" src="static/assets/dmlab_samples/sample_gt_0.mp4" autoplay="" loop="" controls="" muted=""> </video> </th>
                  <th><video data-master-id="master15" width="200" src="static/assets/dmlab_samples/sample_gt_1.mp4" autoplay="" loop="" controls="" muted=""> </video> </th>
                  <th><video data-master-id="master15" width="200" src="static/assets/dmlab_samples/sample_gt_2.mp4" autoplay="" loop="" controls="" muted=""> </video> </th>
                  <th><video data-master-id="master15" width="200" src="static/assets/dmlab_samples/sample_gt_3.mp4" autoplay="" loop="" controls="" muted=""> </video> </th>
                  <th><video data-master-id="master15" width="200" src="static/assets/dmlab_samples/sample_gt_5.mp4" autoplay="" loop="" controls="" muted=""> </video> </th>
                  <th><video data-master-id="master15" width="200" src="static/assets/dmlab_samples/sample_gt_7.mp4" autoplay="" loop="" controls="" muted=""> </video> </th>
                </tr>
              </tbody>
            </table>

            <p style="text-align: center; margin-top: 10px;"><strong>(red boxes: observed context frames, left: prediction, right: ground-truth)</strong></p>

            <div style="text-align: center;">
              <img src="static/assets/dmlab_sample.png" alt="" width="1000" />
            </div>

            <hr>

            <strong>2. FAR achieves state-of-the-art performance on unconditional/conditional video generation, short-video prediction and long-video preidction.</strong>
            
            <br/><br/>

            <div style="text-align: center;">
              <img src="static/assets/result.png" alt="" width="1000" />
            </div>

            <hr>

            <strong>3. FAR demonstrates strong test-time temporal extrapolation performance on both periodical motion and non-periodical motion.
            
            <br/><br/>
            
            <div class="is-centered has-text-centered video-no-container">
              <video preload="auto" poster="" autoplay controls muted loop width="600px" outline="0px"> 
                <source src="static/assets/flexrope_gen.mp4"
                type="video/mp4">
              </video>
              <p class="video-caption" style="text-align: center; margin-top: 10px;">(16x extrapolation, trained on 16 frames while inferring 256 frames, displayed at 4x speed)</p>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!--End paper poster -->

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{gu2025far,
  journal   = {arXiv preprint},
  year      = {2023},
}</code></pre>
  </div>
</section>


<footer class="footer">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>


<script>
  window.onload = function () {
    // Sync function
    function syncVideos(master, slaves) {
      master.addEventListener('play', () => {
        slaves.forEach(video => {
          video.currentTime = master.currentTime;
          video.play();
        });
      });

      master.addEventListener('pause', () => {
        slaves.forEach(video => video.pause());
      });

      master.addEventListener('seeked', () => {
        slaves.forEach(video => {
          video.currentTime = master.currentTime;
        });
      });
    }

    // Sync setup for all masters
    document.querySelectorAll('[id^="master"]').forEach(masterVideo => {
      const masterId = masterVideo.getAttribute('id');
      const slaveVideos = Array.from(document.querySelectorAll(`[data-master-id="${masterId}"]`));
      syncVideos(masterVideo, slaveVideos);
    });
  };
  
</script>

</body>
</html>
